import argparse
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_openai import OpenAI
from get_embedding_function import get_embedding_function
from rank_bm25 import BM25Okapi
import numpy as np

CHROMA_PATH = "chroma"

PROMPT_TEMPLATE = """
Answer the question based only on the following context:

{context}

---

Answer the question based on the above context: {question}
"""


def main():
    # Create CLI.
    parser = argparse.ArgumentParser()
    parser.add_argument("query_text", type=str, help="The query text.")
    args = parser.parse_args()
    query_text = args.query_text
    query_rag(query_text)


def query_rag(query_text: str):
    # Prepare the DB.
    embedding_function = get_embedding_function()
    db = Chroma(persist_directory=CHROMA_PATH,
                embedding_function=embedding_function)

    print("\n=== üîé S√∂kning p√•b√∂rjad ===")
    print(f"Fr√•ga: {query_text}\n")
    
    # H√§mta alla dokument fr√•n databasen f√∂r BM25-s√∂kning.
    print("1. üìö H√§mtar dokument fr√•n databasen...")
    data = db.get()
    documents = data["documents"]  # Textinneh√•llet
    metadatas = data["metadatas"]  # Metadata f√∂r varje dokument
    print(f"   ‚úì Hittade {len(documents)} dokument\n")

    # Bygg BM25-index baserat p√• dokumentinneh√•llet
    tokenized_corpus = [doc.split() for doc in documents]
    bm25 = BM25Okapi(tokenized_corpus)
    query_tokens = query_text.split()
    scores = bm25.get_scores(query_tokens)

    # H√§mta de 12 b√§sta resultaten.
    top_k = 12
    top_indices = np.argsort(scores)[::-1][:top_k]
    results = [({"page_content": documents[i], "metadata": metadatas[i]}, scores[i]) 
               for i in top_indices if scores[i] > 0]
    print(f"2. üîç BM25-s√∂kning klar")
    print(f"   ‚úì Hittade {len(results)} relevanta dokument\n")

    context_text = "\n\n---\n\n".join(
        [doc["page_content"] for doc, _score in results])
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, question=query_text)
    print("3. ü§ñ Genererar svar fr√•n AI-modell...")
    
    try:
        import httpx
        model = OpenAI(
            base_url="http://127.0.0.1:1234/v1",  # LM Studio
            api_key="not-needed",
            temperature=0.7,
            model="meta-llama-3.1-8b-instruct",
            http_client=httpx.Client(timeout=30.0)
        )
        response = model.invoke(prompt)
        response_text = response.strip().replace("<|im_start|>", "").replace("<|im_end|>", "")
        if not response_text:
            response_text = "Error: Received empty response from LLM"
        print("   ‚úì Svar mottaget\n")
    except Exception as e:
        print(f"\n‚ùå Fel vid anslutning till LM Studio: {str(e)}")
        print("Kontrollera att LM Studio k√∂rs och att modellen √§r laddad")
        return "Error: Could not connect to LM Studio"

    sources = [f"- {doc['metadata'].get('source', 'Unknown')} (sida {doc['metadata'].get('page', '?'):.0f})" 
              for doc, _score in results]
    
    print("\n=== ‚ú® RESULTAT ===")
    print("\nüìù SVAR:")
    print(response_text)
    print("\nüìö K√ÑLLOR:")
    print("\n".join(sources))
    print("\n================")


if __name__ == "__main__":
    main()
